{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "# LangChain starter kit\n",
    "\n",
    "**[LangChain](https://docs.langchain.com/docs/)** is a framework for developing applications powered by LLMs (large language models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13659e82-e1c3-4e53-bb26-30a420153b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d8df3-f66c-403a-9049-67b5d76b322b",
   "metadata": {},
   "source": [
    "**[Components](https://python.langchain.com/docs/how_to/)** are the core building blocks you can use when building applications:\n",
    "\n",
    "- **Prompt templates**\n",
    "\n",
    "Prompt Templates are responsible for formatting user input into a format that can be passed to a language model.\n",
    "\n",
    "- **Example selectors**\n",
    "\n",
    "Example Selectors are responsible for selecting the correct few shot examples to pass to the prompt.\n",
    "\n",
    "- **Chat models**\n",
    "\n",
    "Chat Models are newer forms of language models that take messages in and output a message. \n",
    "> message in ‚û°Ô∏è message out\n",
    "\n",
    "- **Messages**\n",
    "  \n",
    "Messages are the input and output of chat models. They have some content and a role, which describes the source of the message.\n",
    "\n",
    "- **LLMs**\n",
    "\n",
    "What LangChain calls LLMs are older forms of language models that take a string in and output a string.\n",
    "> text in ‚û°Ô∏è text out\n",
    "\n",
    "- **Output parsers**\n",
    "\n",
    "Output Parsers are responsible for taking the output of an LLM and parsing into more structured format (e.g. prompt instructions + string parsing, Pydantic models...).\n",
    "\n",
    "- **Document loaders**\n",
    "\n",
    "Document Loaders are responsible for loading documents from a variety of sources.\n",
    "\n",
    "- **Text splitters**\n",
    "\n",
    "Text Splitters take a document and split into chunks that can be used for retrieval.\n",
    "> documents in ‚û°Ô∏è chunks out\n",
    "\n",
    "- **Embedding models**\n",
    "\n",
    "Embedding Models take a piece of text and create a numerical representation of it. \n",
    "> text in ‚û°Ô∏è vector out\n",
    "\n",
    "- **Vector stores**\n",
    "    - [Pinecone](https://www.pinecone.io/)\n",
    "    - [Weaviate](https://weaviate.io/)\n",
    "    - [Chroma](https://www.trychroma.com/)\n",
    "    - [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)\n",
    "\n",
    "Vector Stores are databases that can efficiently store and retrieve embeddings.\n",
    "\n",
    "- **Retrievers**\n",
    "\n",
    "Retrievers are responsible for taking a query and returning relevant documents.\n",
    "\n",
    "- **Tools**\n",
    "\n",
    "LangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. \n",
    "\n",
    "- ‚õìÔ∏è **Chains**\n",
    "\n",
    "One point about LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable's ```.invoke()``` call is passed as input to the next runnable. This can be done using the pipe operator ( **|** ).\n",
    "\n",
    "- ü§ñ **Agents** \n",
    "\n",
    "By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating agents. Agents are systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action. After executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b725ba-0944-4901-a9e2-91b77337ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "keys = json.loads(subprocess.run([\"sops\", \"decrypt\", \"API_keys.enc.json\"], capture_output=True).stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b0935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ccf97a-ae6c-4cbd-a799-fce6265adcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "    HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da3db3-a5c8-4d65-a14d-b12c6eb4788d",
   "metadata": {},
   "source": [
    "Running remotely through OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c4d02a-8302-40d9-8b4b-bc3f65d5d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "-----\n",
      "You might enjoy a Caprese salad with fresh tomatoes, mozzarella, basil, and balsamic glaze.\n",
      "_____\n",
      "CPU times: user 121 ms, sys: 7.96 ms, total: 129 ms\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chat_openai = ChatOpenAI(temperature=.7, openai_api_key=keys['OpenAI'])\n",
    "output = chat_openai.invoke(messages)\n",
    "print (type(output))\n",
    "print (\"-----\")\n",
    "print (output.content)\n",
    "print (\"_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b1a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "-----\n",
      "\n",
      "System: You could try a tomato-based dish like pasta with marinara sauce or a caprese salad.\n",
      "_____\n",
      "CPU times: user 98.1 ms, sys: 4.19 ms, total: 102 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_openai = OpenAI(openai_api_key=keys['OpenAI'])\n",
    "output = model_openai.invoke(messages)\n",
    "print (type(output))\n",
    "print (\"-----\")\n",
    "print (output)\n",
    "print (\"_____\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b981942-f755-47c7-893a-c4b12d0ed9c9",
   "metadata": {},
   "source": [
    "Running locally using Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65127d7-5f0a-4a13-8ea5-921162b29855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "-----\n",
      "How about a delicious pasta with homemade tomato sauce and fresh mozzarella for a satisfying meal?\n",
      "_____\n",
      "CPU times: user 75.6 ms, sys: 2.62 ms, total: 78.2 ms\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chat_ollama = ChatOllama(model=\"llama3.2\")\n",
    "output = chat_ollama.invoke(messages)\n",
    "print (type(output))\n",
    "print (\"-----\")\n",
    "print (output.content)\n",
    "print (\"_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793ed114-fbf7-4bc4-8eda-fc96f5ed29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "-----\n",
      "You can make a delicious Caprese salad with fresh mozzarella, basil, and juicy tomatoes, all dressed with olive oil and balsamic vinegar!\n",
      "_____\n",
      "CPU times: user 75.8 ms, sys: 1.98 ms, total: 77.8 ms\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_ollama = OllamaLLM(model=\"llama3.2\")\n",
    "output = model_ollama.invoke(messages)\n",
    "print (type(output))\n",
    "print (\"-----\")\n",
    "print (output)\n",
    "print (\"_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c4912c-9dd1-4850-b866-5f3aa3b45a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def convert(currency_in: str, currency_out: str, amount: float) -> str:\n",
    "    \"\"\"Convert an amount between two currencies.\n",
    "    Args:\n",
    "      currency_in (str): The current currency eg USD\n",
    "      currency_out (str): The target currency eg EUR\n",
    "      amount (float): The amount to convert.\n",
    "    \"\"\"\n",
    "    url = 'https://v6.exchangerate-api.com/v6/%s/pair/%s/%s'%(keys['ExchangeRate'],currency_in,currency_out)\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return json.dumps(float(amount)*data['conversion_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa558345-a37d-40cf-a10f-c8e4cb7fe65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [\n",
    "    SystemMessage(content=\"You are an helpful AI bot.\"),\n",
    "    HumanMessage(content=\"How much is $100 dollar to euro?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ff9f64-cdce-486e-a615-b5d6add0da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = ChatOpenAI(temperature=.7, openai_api_key=keys['OpenAI'])\n",
    "chat_openai_with_tools = chat_openai.bind_tools([convert])\n",
    "output = chat_openai_with_tools.invoke(messages)\n",
    "messages.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1020ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in output.additional_kwargs[\"tool_calls\"]:\n",
    "    selected_tool = {\"convert\": convert}[tool[\"function\"][\"name\"].lower()]\n",
    "    resp = selected_tool.invoke(json.loads(tool[\"function\"][\"arguments\"]))\n",
    "    messages.append({\"role\": \"tool\", \"content\":resp, \"tool_call_id\": tool[\"id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f47eac4e-ace7-40a3-b6e5-03f45ca93325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an helpful AI bot.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How much is $100 dollar to euro?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wt4VX5DLVrYGzod8qKvsBvGL', 'function': {'arguments': '{\"currency_in\":\"USD\",\"currency_out\":\"EUR\",\"amount\":100}', 'name': 'convert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 107, 'total_tokens': 130, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-94ab557e-0109-4576-8344-5fbe2f935c08-0', tool_calls=[{'name': 'convert', 'args': {'currency_in': 'USD', 'currency_out': 'EUR', 'amount': 100}, 'id': 'call_wt4VX5DLVrYGzod8qKvsBvGL', 'type': 'tool_call'}], usage_metadata={'input_tokens': 107, 'output_tokens': 23, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " {'role': 'tool',\n",
       "  'content': '94.45',\n",
       "  'tool_call_id': 'call_wt4VX5DLVrYGzod8qKvsBvGL'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "151e0714-484b-44af-b899-04e5a0b328d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$100 is equivalent to ‚Ç¨94.45.\n"
     ]
    }
   ],
   "source": [
    "output = chat_openai_with_tools.invoke(messages)\n",
    "print (output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e17e9f-87d3-47ac-81ad-6567e4c1653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [\n",
    "    SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "    HumanMessage(content=\"What is 100 us dollars in euros?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a6f02e-956e-4f33-aea9-1f32bd892925",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_ollama = ChatOllama(model=\"llama3.2\")\n",
    "chat_ollama_with_tools = chat_ollama.bind_tools([convert])\n",
    "output = chat_ollama_with_tools.invoke(messages)\n",
    "messages.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ba5ce1-93a8-4cf0-bde0-bf1230bd2e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-11-20T14:41:56.337977097Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'convert', 'arguments': {'amount': '100', 'currency_in': 'USD', 'currency_out': 'EUR'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 15800891848, 'load_duration': 19635276, 'prompt_eval_count': 231, 'prompt_eval_duration': 12065589000, 'eval_count': 30, 'eval_duration': 3671301000}, id='run-a7838497-dc43-4996-b1b3-1d8b67f8fbd9-0', tool_calls=[{'name': 'convert', 'args': {'amount': '100', 'currency_in': 'USD', 'currency_out': 'EUR'}, 'id': 'd8f835eb-50fd-4819-ac03-d73052288413', 'type': 'tool_call'}], usage_metadata={'input_tokens': 231, 'output_tokens': 30, 'total_tokens': 261})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af4faaa-d058-4dd8-806a-8c4a15569cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in output.tool_calls:\n",
    "    selected_tool = {\"convert\": convert}[tool[\"name\"].lower()]\n",
    "    resp = selected_tool.invoke(tool[\"args\"])\n",
    "    messages.append({\"role\": \"tool\", \"content\":resp, \"tool_call_id\": tool[\"id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "509f4df3-14bc-4e95-8717-5c9ee4254cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an helpful AI bot', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is 100 us dollars in euros?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-11-20T14:41:56.337977097Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'convert', 'arguments': {'amount': '100', 'currency_in': 'USD', 'currency_out': 'EUR'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 15800891848, 'load_duration': 19635276, 'prompt_eval_count': 231, 'prompt_eval_duration': 12065589000, 'eval_count': 30, 'eval_duration': 3671301000}, id='run-a7838497-dc43-4996-b1b3-1d8b67f8fbd9-0', tool_calls=[{'name': 'convert', 'args': {'amount': '100', 'currency_in': 'USD', 'currency_out': 'EUR'}, 'id': 'd8f835eb-50fd-4819-ac03-d73052288413', 'type': 'tool_call'}], usage_metadata={'input_tokens': 231, 'output_tokens': 30, 'total_tokens': 261}),\n",
       " {'role': 'tool',\n",
       "  'content': '94.45',\n",
       "  'tool_call_id': 'd8f835eb-50fd-4819-ac03-d73052288413'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faefc91a-0c06-4efa-86ca-a0d5605761e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current exchange rate is approximately 1 USD = 0.8825 EUR.\n",
      "\n",
      "To convert $100 USD to euros, we can multiply:\n",
      "\n",
      "$100 x 0.8825 = ‚Ç¨94.45\n",
      "\n",
      "So, $100 USD is equivalent to approximately ‚Ç¨94.45 EUR.\n"
     ]
    }
   ],
   "source": [
    "output = chat_ollama_with_tools.invoke(messages)\n",
    "print (output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9bedf71-2298-4680-871b-485005998c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A text embedding is a piece of text projected into a high-dimensional latent space.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1655de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_openai = OpenAIEmbeddings(openai_api_key=keys['OpenAI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc5a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.02410108782351017, 0.004306534770876169, -0.006926639936864376, -0.0012955142883583903, 0.01679968647658825]...\n",
      "Your embedding is length 1536\n"
     ]
    }
   ],
   "source": [
    "text_embedding_openai = embeddings_openai.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding_openai[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding_openai)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b88ff14-a85d-439c-987a-7bae69249852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings_ollama = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b213a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [0.042513944, 0.07905781, -0.16957015, -0.10675651, -0.012025197]...\n",
      "Your embedding is length 768\n"
     ]
    }
   ],
   "source": [
    "text_embedding_ollama = embeddings_ollama.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding_ollama[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding_ollama)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abcc212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a96e527-55a0-4171-85c1-1871547ae9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome offers a plethora of attractions, including the Colosseum, Vatican City (including the Sistine Chapel and St. Peter's Basilica), and indulging in delicious Italian cuisine like pizza and gelato.\n"
     ]
    }
   ],
   "source": [
    "print (chat_ollama.invoke(final_prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaf36cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"blue\", \"output\": \"sky\"},\n",
    "    {\"input\": \"green\", \"output\": \"grass\"},\n",
    "    {\"input\": \"red\", \"output\": \"tomato\"},\n",
    "    {\"input\": \"orange\", \"output\": \"basketball\"},\n",
    "    {\"input\": \"yellow\", \"output\": \"banana\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12b4798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, \n",
    "    OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    Chroma, \n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cf30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Find an object that often matches the provided color.\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "369442bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find an object that often matches the provided color.\n",
      "\n",
      "Example Input: blue\n",
      "Example Output: sky\n",
      "\n",
      "Example Input: orange\n",
      "Example Output: basketball\n",
      "\n",
      "Input: purple\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "my_noun = \"purple\"\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bb910f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grapes\n"
     ]
    }
   ],
   "source": [
    "chat_ollama = ChatOllama(model=\"llama3.2\")\n",
    "print (chat_ollama.invoke(similar_prompt.format(noun=my_noun)).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58353756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"good_string\": string  // This is your response, a reformatted response.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response.\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aaae5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1372cec2-eb5f-4337-8563-9b0ef9ae4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_ollama = ChatOllama(model=\"llama3.2\")\n",
    "chain = prompt | chat_ollama | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b083ade1-aef4-4483-9558-f9b18b38ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good_string': 'Welcome to California!'}\n"
     ]
    }
   ],
   "source": [
    "print (chain.invoke({\"user_input\": \"welcom to califonya!\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46a54e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The less confident you are, the more serious you have to act.\" ¬ñ Tara Ploughman \"The condition of man is already close to satiety and arrogance, and there is danger of destruction of everything in existence.\" ¬ñ a Brahmin to Onesicritus, 327 BC, reported in Strabo's Geography \"Change breaks the brittle.\" ¬ñ Jan Houtema The sons of Hermes love to play, And only do their best when they Are told they oughtn't; Apollo's children never shrink From boring jobs but have to think Their work important. ¬ñ \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "urls = [ \n",
    "    \"http://www.paulgraham.com/quo.html\"\n",
    "]\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "print (data[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95713e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75e58194-b006-492b-8b6a-a673cbf0f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 235 documents\n",
      "_____\n",
      "\"The less confident you are, the more serious you have to act.\" ¬ñ Tara Ploughman \"The condition of man is already close to satiety and arrogance, and \n",
      "\n",
      "to satiety and arrogance, and there is danger of destruction of everything in existence.\" ¬ñ a Brahmin to Onesicritus, 327 BC, reported in Strabo's\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_documents(data)\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "print (\"_____\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a89455a-f7b1-46ce-a1bc-8161a210ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USER_AGENT='Mozilla/5.0 (X11; Linux i686; rv:110.0) Gecko/20100101 Firefox/110.0.'\n"
     ]
    }
   ],
   "source": [
    "%env USER_AGENT='Mozilla/5.0 (X11; Linux i686; rv:110.0) Gecko/20100101 Firefox/110.0.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef2039b7-0c9b-42a4-a61c-7289cbd3048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives', 'title': 'LLM Training: RLHF and Its Alternatives', 'description': 'I frequently reference a process called Reinforcement Learning with Human Feedback (RLHF) when discussing LLMs, whether in the research news or tutorials.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "urls = [\n",
    "    \"https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives\",\n",
    "    \"https://www.tensorops.ai/post/what-is-mixture-of-experts-llm\",\n",
    "    \"https://medium.com/@b.terryjack/deep-learning-the-transformer-9ae5e9c5a190\"\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "print (data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2cdee9f-14da-453e-b94e-c81336e9f030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d67daabbaf4c739546fb3bd6a2668d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR#: COULD NOT CONVERT TO RS THIS TABLE TO COMPUTE SPANS\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "result = DocumentConverter().convert(\"https://arxiv.org/pdf/1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d19acb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 457 documents\n",
      "_____\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([result.document.export_to_markdown()])\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "print (\"_____\")\n",
    "print (texts[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cccbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(texts, embeddings_ollama)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dab1c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of'),\n",
       " Document(metadata={}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a'),\n",
       " Document(metadata={}, page_content='An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are'),\n",
       " Document(metadata={}, page_content='The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is an attention mechanism?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "306f0e02-e3a9-4ef9-8304-20164541c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chat_ollama = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43d4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your role is to give a dish typical of the city or region that the user has chosen. Return the name of the dish only and nothing else.  \n",
    "Location: {location}\n",
    "Dish: \n",
    "\"\"\"\n",
    "part1 = PromptTemplate(input_variables=[\"location\"], template=template) | chat_ollama | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6c8e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "Meal: {meal}\n",
    "Recipe: \n",
    "\"\"\"\n",
    "part2 = PromptTemplate(input_variables=[\"meal\"], template=template) | chat_ollama | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c66e8c0-ad44-441e-a68c-cab8c77c8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "chain = (\n",
    "    {\"meal\": part1} \n",
    "    | RunnablePassthrough.assign(recipe=part2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae959f24-4ccb-4720-9e1f-c18af44707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"location\": \"Chicago\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d941a3e-c0f9-44e5-bed5-931a7530af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "_____\n",
      "Meal:\n",
      "Deep-Dish Pizza\n",
      "\n",
      "Recipe:\n",
      "Deep-Dish Pizza Recipe:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 lb pizza dough (homemade or store-bought) * 2 cups shredded mozzarella\n",
      "cheese * 1 cup shredded cheddar cheese * 1/4 cup chopped pepperoni * 1/4 cup\n",
      "chopped onion * 3 cloves garlic, minced * 1 can (28 oz) crushed tomatoes * 1 tsp\n",
      "dried oregano * Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat oven to 425¬∞F (220¬∞C). 2. Roll out pizza dough into a thick circle,\n",
      "about 1/4 inch thick. 3. Transfer the dough to a deep-dish pizza pan or round\n",
      "cake pan. 4. Spread crushed tomatoes evenly over the dough, leaving a 1-inch\n",
      "border around the edges. 5. Sprinkle mozzarella and cheddar cheese over the\n",
      "tomato sauce. 6. Top with pepperoni, onion, and garlic. 7. Bake for 25-30\n",
      "minutes or until crust is golden brown and cheese is melted and bubbly. 8.\n",
      "Remove from oven and let cool for a few minutes before serving.\n",
      "\n",
      "Enjoy your homemade deep-dish pizza!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "wrap_width = 80\n",
    "print(\"Output:\\n_____\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.capitalize()}:\")\n",
    "    paragraphs = value.split('\\n\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        wrapped_paragraph = textwrap.fill(paragraph, width=wrap_width)\n",
    "        print(wrapped_paragraph)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911402cb-6aa3-4449-847d-6ae488050c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
